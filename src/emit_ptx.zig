const std = @import("std");
const lexer = @import("lexer.zig");
const parser = @import("parser.zig");

// MLIR C API bindings
const MLIR = @cImport({
    @cInclude("mlir-c/Support.h"); // Must come first - defines MlirStringRef
    @cInclude("mlir-c/IR.h");
    @cInclude("mlir-c/BuiltinTypes.h");
    @cInclude("mlir-c/BuiltinAttributes.h");
    @cInclude("mlir-c/Dialect/Func.h");
    @cInclude("mlir-c/Dialect/GPU.h");
    @cInclude("mlir-c/Dialect/Arith.h");
    @cInclude("mlir-c/Dialect/MemRef.h");
    @cInclude("mlir-c/Dialect/SCF.h");
    @cInclude("mlir-c/Dialect/ControlFlow.h");
    @cInclude("mlir-c/Pass.h");
    @cInclude("mlir-c/Conversion.h");
    @cInclude("mlir-c/Transforms.h");
    @cInclude("mlir-c/RegisterEverything.h"); // For mlirRegisterAllDialects
    @cInclude("mlir-c/Target/LLVMIR.h"); // For mlirTranslateModuleToLLVMIR
    @cInclude("llvm-c/Core.h"); // For LLVM IR manipulation
    @cInclude("gpu_to_nvvm_wrapper.h"); // Our custom wrapper for GPU to NVVM with options
});

const EmitPtxError = error{
    FileNotFound,
    ParseError,
    FunctionNotFound,
    PipelineError,
    InvalidArguments,
    OutputFileError,
} || std.mem.Allocator.Error;

const Args = struct {
    input_file: []const u8,
    function_name: []const u8,
    sm_version: u32 = 50, // Default to SM 5.0
    verbose: bool = false,
    help: bool = false,
};

pub fn main() !void {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    // Parse command line arguments
    var output_file_path: ?[]const u8 = null;
    const args = try parseArgs(allocator, &output_file_path);
    defer if (output_file_path) |path| allocator.free(path);

    if (args.help) {
        printUsage();
        return;
    }

    // Clean up duplicated strings at the end
    defer allocator.free(args.input_file);
    defer allocator.free(args.function_name);

    if (args.verbose) {
        std.debug.print("Emit PTX Tool (using MLIR pipeline)\n", .{});
        std.debug.print("Input file: {s}\n", .{args.input_file});
        std.debug.print("Function: {s}\n", .{args.function_name});
        std.debug.print("SM version: {d}\n", .{args.sm_version});
        if (output_file_path) |output_file| {
            std.debug.print("Output file: {s}\n", .{output_file});
        } else {
            std.debug.print("Output: stdout\n", .{});
        }
    }

    // Try to generate PTX using our pipeline
    const ptx_code = generatePTXFromPipeline(allocator, args) catch |err| {
        if (args.verbose) {
            std.debug.print("Pipeline failed: {}, falling back to static GPU MLIR\n", .{err});
        }
        const fallback_ptx = try generatePTXFromStatic(allocator, args);
        defer allocator.free(fallback_ptx);

        // Generate PTX header
        const header = try std.fmt.allocPrint(allocator, "// Generated PTX for function: {s}\n// Target: SM {d}\n// Generated by DCC emit_ptx tool using MLIR C API (no mlir-opt or mlir-translate calls) (fallback)\n\n", .{ args.function_name, args.sm_version });
        defer allocator.free(header);

        // Output the PTX code
        try outputPTX(allocator, output_file_path, header, fallback_ptx, args.verbose);
        return;
    };
    defer allocator.free(ptx_code);

    // Generate PTX header
    const header = try std.fmt.allocPrint(allocator, "// Generated PTX for function: {s}\n// Target: SM {d}\n// Generated by DCC emit_ptx tool using MLIR C API (no mlir-opt or mlir-translate calls)\n\n", .{ args.function_name, args.sm_version });
    defer allocator.free(header);

    // Output the PTX code
    try outputPTX(allocator, output_file_path, header, ptx_code, args.verbose);
}

fn outputPTX(allocator: std.mem.Allocator, output_file: ?[]const u8, header: []const u8, ptx_code: []const u8, verbose: bool) !void {
    const full_content = try std.fmt.allocPrint(allocator, "{s}{s}", .{ header, ptx_code });
    defer allocator.free(full_content);

    if (output_file) |file_path| {
        // Write to file
        std.fs.cwd().writeFile(.{ .sub_path = file_path, .data = full_content }) catch |err| {
            std.debug.print("Error writing to output file '{s}': {}\n", .{ file_path, err });
            return EmitPtxError.OutputFileError;
        };
        if (verbose) {
            std.debug.print("✅ PTX written to: {s}\n", .{file_path});
        }
    } else {
        // Write to stdout
        std.debug.print("{s}", .{full_content});
    }
}

fn generatePTXFromPipeline(allocator: std.mem.Allocator, args: Args) ![]const u8 {
    if (args.verbose) {
        std.debug.print("Attempting to parse input file and generate custom GPU MLIR...\n", .{});
    }

    // Read and parse input file
    const source_code = std.fs.cwd().readFileAlloc(allocator, args.input_file, 1024 * 1024) catch |err| {
        switch (err) {
            error.FileNotFound => {
                std.debug.print("Error: File '{s}' not found\n", .{args.input_file});
                return EmitPtxError.FileNotFound;
            },
            else => return err,
        }
    };
    defer allocator.free(source_code);

    // Tokenize
    var lex = lexer.Lexer.init(allocator, source_code);
    const tokens = try lex.tokenize();
    defer allocator.free(tokens);

    // Parse
    var parse = parser.Parser.init(allocator, tokens, source_code);
    const ast = parse.parse() catch |err| {
        std.debug.print("Parse error: {}\n", .{err});
        return EmitPtxError.ParseError;
    };
    defer parser.freeAST(allocator, ast);

    // Find the specified function
    const func_decl = try findFunction(ast, args.function_name);

    if (args.verbose) {
        std.debug.print("Found function: {s}\n", .{func_decl.name});
    }

    // For now, if we reach here, we'll fall back to static
    // TODO: Implement dynamic GPU MLIR generation from AST
    return EmitPtxError.PipelineError;
}

fn generatePTXFromStatic(allocator: std.mem.Allocator, args: Args) ![]const u8 {
    if (args.verbose) {
        std.debug.print("Using static GPU MLIR from simple_vector_add_gpu.mlir\n", .{});
    }

    // Steps 1-7: Canonicalize, GPU kernel outlining, SCF to CF, GPU to NVVM, NVVM to LLVM, finalize MemRef, convert Func, and reconcile unrealized casts using MLIR C API
    if (args.verbose) std.debug.print("📝 Running integrated MLIR passes (canonicalize, GPU kernel outlining, SCF to CF, GPU to NVVM with bare ptr, NVVM to LLVM, finalize MemRef, convert Func to LLVM with bare ptr, reconcile unrealized casts)...\n", .{});
    const transformed_mlir = try canonicalizeMLIRFile(allocator, "simple_vector_add_gpu.mlir", args.verbose);
    defer allocator.free(transformed_mlir);

    // Steps 1-10 are now integrated: MLIR passes + MLIR→LLVM translation via MLIR C API

    // Step 8: Extract kernel function for standalone compilation
    if (args.verbose) std.debug.print("🔧 Extracting kernel function...\n", .{});
    const standalone_kernel = try extractKernelFunction(allocator, transformed_mlir);
    defer allocator.free(standalone_kernel);

    // Step 9: Fix NVVM operations for mlir-translate
    if (args.verbose) std.debug.print("🔧 Fixing NVVM operations...\n", .{});
    const fixed_mlir = try fixNVVMOperations(allocator, standalone_kernel);
    defer allocator.free(fixed_mlir);

        // Step 10: Translate MLIR to LLVM IR using MLIR C API (replaces external mlir-translate)
    const llvm_ir_content = try translateMLIRToLLVMIR(allocator, fixed_mlir, args.verbose);
    defer allocator.free(llvm_ir_content);
    
    // Write LLVM IR to file for llc step
    std.fs.cwd().writeFile(.{ .sub_path = "kernel.ll", .data = llvm_ir_content }) catch |err| {
        std.debug.print("Error writing LLVM IR file: {}\n", .{err});
        return EmitPtxError.PipelineError;
    };

    // Step 11: Compile LLVM IR to PTX using llc
    if (args.verbose) std.debug.print("🎯 Compiling LLVM IR to PTX...\n", .{});
    const mcpu_arg = try std.fmt.allocPrint(allocator, "-mcpu=sm_{d}", .{args.sm_version});
    defer allocator.free(mcpu_arg);

    try runCommand(allocator, &[_][]const u8{ "llc", "kernel.ll", "-march=nvptx64", mcpu_arg, "-o", "kernel.ptx" });

    // Step 12: Read the generated PTX
    if (args.verbose) std.debug.print("📖 Reading generated PTX...\n", .{});
    const ptx_content = std.fs.cwd().readFileAlloc(allocator, "kernel.ptx", 1024 * 1024) catch |err| {
        std.debug.print("Error reading generated PTX: {}\n", .{err});
        return EmitPtxError.PipelineError;
    };

    // Clean up temporary files
    if (!args.verbose) {
        std.fs.cwd().deleteFile("kernel.ll") catch {};
        std.fs.cwd().deleteFile("kernel.ptx") catch {};
    }

    if (args.verbose) std.debug.print("✅ PTX generation complete\n", .{});
    return ptx_content;
}

fn runCommand(allocator: std.mem.Allocator, argv: []const []const u8) !void {
    const result = std.process.Child.run(.{
        .allocator = allocator,
        .argv = argv,
    }) catch |err| {
        std.debug.print("Error running command: {}\n", .{err});
        return EmitPtxError.PipelineError;
    };
    defer allocator.free(result.stdout);
    defer allocator.free(result.stderr);

    if (result.term != .Exited or result.term.Exited != 0) {
        std.debug.print("Command failed: {s}\n", .{argv[0]});
        if (result.stderr.len > 0) {
            std.debug.print("Error: {s}\n", .{result.stderr});
        }
        return EmitPtxError.PipelineError;
    }
}

fn extractKernelFunction(allocator: std.mem.Allocator, input_content: []const u8) ![]const u8 {

    // Find the kernel function and extract it
    const kernel_start = std.mem.indexOf(u8, input_content, "llvm.func @gpu_add_kernel") orelse {
        std.debug.print("Error: Could not find kernel function\n", .{});
        return EmitPtxError.PipelineError;
    };

    const kernel_end = std.mem.indexOf(u8, input_content[kernel_start..], "  }") orelse {
        std.debug.print("Error: Could not find end of kernel function\n", .{});
        return EmitPtxError.PipelineError;
    };

    // Extract the kernel function
    const kernel_function = input_content[kernel_start .. kernel_start + kernel_end + 3];

    // Remove the gpu.kernel attribute as it's not needed in standalone module
    const kernel_with_attrs = std.mem.replacementSize(u8, kernel_function, "gpu.kernel, ", "");
    const cleaned_kernel = try allocator.alloc(u8, kernel_with_attrs);
    defer allocator.free(cleaned_kernel);
    _ = std.mem.replace(u8, kernel_function, "gpu.kernel, ", "", cleaned_kernel);

    // Also remove if it's at the end
    const kernel_with_attrs2 = std.mem.replacementSize(u8, cleaned_kernel, ", gpu.kernel", "");
    const cleaned_kernel2 = try allocator.alloc(u8, kernel_with_attrs2);
    defer allocator.free(cleaned_kernel2);
    _ = std.mem.replace(u8, cleaned_kernel, ", gpu.kernel", "", cleaned_kernel2);

    // Final cleanup - use the final cleaned version
    const final_kernel = try allocator.dupe(u8, cleaned_kernel2);

    // Create a standalone module with the kernel function
    const standalone_module = try std.fmt.allocPrint(allocator,
        \\module attributes {{nvvm.target = "cuda"}} {{
        \\  {s}
        \\  llvm.func @llvm.nvvm.read.ptx.sreg.tid.x() -> i32 attributes {{passthrough = ["nounwind", "readnone"]}}
        \\  llvm.func @llvm.nvvm.read.ptx.sreg.ntid.x() -> i32 attributes {{passthrough = ["nounwind", "readnone"]}}
        \\  llvm.func @llvm.nvvm.read.ptx.sreg.ctaid.x() -> i32 attributes {{passthrough = ["nounwind", "readnone"]}}
        \\}}
        \\
    , .{final_kernel});
    defer allocator.free(final_kernel);

    return standalone_module;
}

fn fixNVVMOperations(allocator: std.mem.Allocator, input_content: []const u8) ![]const u8 {

    // Replace NVVM operations with LLVM intrinsic calls
    const content1 = std.mem.replaceOwned(u8, allocator, input_content, "nvvm.read.ptx.sreg.ctaid.x : i32", "llvm.call @llvm.nvvm.read.ptx.sreg.ctaid.x() : () -> i32") catch |err| {
        std.debug.print("Error replacing NVVM operations: {}\n", .{err});
        return EmitPtxError.PipelineError;
    };
    defer allocator.free(content1);

    const content2 = std.mem.replaceOwned(u8, allocator, content1, "nvvm.read.ptx.sreg.ntid.x : i32", "llvm.call @llvm.nvvm.read.ptx.sreg.ntid.x() : () -> i32") catch |err| {
        std.debug.print("Error replacing NVVM operations: {}\n", .{err});
        return EmitPtxError.PipelineError;
    };
    defer allocator.free(content2);

    const content3 = std.mem.replaceOwned(u8, allocator, content2, "nvvm.read.ptx.sreg.tid.x : i32", "llvm.call @llvm.nvvm.read.ptx.sreg.tid.x() : () -> i32") catch |err| {
        std.debug.print("Error replacing NVVM operations: {}\n", .{err});
        return EmitPtxError.PipelineError;
    };
    defer allocator.free(content3);

    // Remove invalid function attributes and nvvm.target
    const content4 = std.mem.replaceOwned(u8, allocator, content3, "attributes {passthrough = [\"nounwind\", \"readnone\"]}", "") catch |err| {
        std.debug.print("Error removing invalid attributes: {}\n", .{err});
        return EmitPtxError.PipelineError;
    };
    defer allocator.free(content4);

    const clean_content = std.mem.replaceOwned(u8, allocator, content4, "module attributes {nvvm.target = \"cuda\"} {", "module {") catch |err| {
        std.debug.print("Error removing nvvm.target: {}\n", .{err});
        return EmitPtxError.PipelineError;
    };

    return clean_content;
}

/// Translate MLIR to LLVM IR using the MLIR C API (replaces external mlir-translate)
fn translateMLIRToLLVMIR(allocator: std.mem.Allocator, mlir_content: []const u8, verbose: bool) ![]const u8 {
    if (verbose) {
        std.debug.print("🔄 Translating MLIR to LLVM IR using MLIR C API...\n", .{});
    }

    // Create null-terminated version for MLIR C API
    const null_terminated_content = try allocator.allocSentinel(u8, mlir_content.len, 0);
    defer allocator.free(null_terminated_content);
    @memcpy(null_terminated_content, mlir_content);

    // Initialize MLIR context (translations will register required dialects)
    const context = MLIR.mlirContextCreate();
    defer MLIR.mlirContextDestroy(context);

    // Register LLVM translations (this will register required dialects automatically)
    MLIR.mlirRegisterAllLLVMTranslations(context);

    if (verbose) {
        std.debug.print("✅ Created MLIR context for translation\n", .{});
    }

    // Parse the MLIR content
    const input_str_ref = MLIR.mlirStringRefCreateFromCString(null_terminated_content);
    const module = MLIR.mlirModuleCreateParse(context, input_str_ref);

    if (MLIR.mlirModuleIsNull(module)) {
        if (verbose) {
            std.debug.print("❌ Failed to parse MLIR for translation\n", .{});
        }
        return EmitPtxError.PipelineError;
    }
    defer MLIR.mlirModuleDestroy(module);

    if (verbose) {
        std.debug.print("✅ Successfully parsed MLIR for translation\n", .{});
    }

    // Get module operation
    const module_op = MLIR.mlirModuleGetOperation(module);

    // Create LLVM context for translation
    const llvm_context = MLIR.LLVMContextCreate();
    defer MLIR.LLVMContextDispose(llvm_context);

    // Translate MLIR to LLVM IR
    const llvm_module = MLIR.mlirTranslateModuleToLLVMIR(module_op, llvm_context);
    if (llvm_module == null) {
        if (verbose) {
            std.debug.print("❌ Failed to translate MLIR to LLVM IR\n", .{});
        }
        return EmitPtxError.PipelineError;
    }
    defer MLIR.LLVMDisposeModule(llvm_module);

    if (verbose) {
        std.debug.print("✅ Successfully translated MLIR to LLVM IR\n", .{});
    }

    // Convert LLVM module to string
    const llvm_ir_cstr = MLIR.LLVMPrintModuleToString(llvm_module);
    defer MLIR.LLVMDisposeMessage(llvm_ir_cstr);

    // Copy to our allocator-managed memory
    const llvm_ir_len = std.mem.len(llvm_ir_cstr);
    const llvm_ir_content = try allocator.alloc(u8, llvm_ir_len);
    @memcpy(llvm_ir_content, llvm_ir_cstr[0..llvm_ir_len]);

    if (verbose) {
        std.debug.print("✅ Converted LLVM module to string ({d} bytes)\n", .{llvm_ir_len});
    }

    return llvm_ir_content;
}

/// Run integrated MLIR passes: canonicalize, GPU kernel outlining, SCF to CF, GPU to NVVM (with bare ptr), NVVM to LLVM, finalize MemRef to LLVM, convert Func to LLVM (with bare ptr), and reconcile unrealized casts
fn canonicalizeMLIRFile(allocator: std.mem.Allocator, input_file: []const u8, verbose: bool) ![]const u8 {
    if (verbose) {
        std.debug.print("🔧 Steps 1-7: Integrated MLIR passes using C API\n", .{});
    }

    // Read the input MLIR file for practical output
    const input_content = std.fs.cwd().readFileAlloc(allocator, input_file, 1024 * 1024) catch |err| {
        std.debug.print("Error reading input MLIR file: {}\n", .{err});
        return EmitPtxError.PipelineError;
    };
    defer allocator.free(input_content);

    // Create null-terminated version for MLIR C API (C APIs expect null termination)
    const null_terminated_content = try allocator.allocSentinel(u8, input_content.len, 0);
    defer allocator.free(null_terminated_content);
    @memcpy(null_terminated_content, input_content);

    // Use bulk registration for better compatibility
    const registry = MLIR.mlirDialectRegistryCreate();
    if (verbose) {
        std.debug.print("✅ Registry created\n", .{});
    }
    defer MLIR.mlirDialectRegistryDestroy(registry);

    // Register only the specific dialects we need (avoid initialization conflicts)
    MLIR.mlirDialectHandleInsertDialect(MLIR.mlirGetDialectHandle__func__(), registry);
    MLIR.mlirDialectHandleInsertDialect(MLIR.mlirGetDialectHandle__gpu__(), registry);
    MLIR.mlirDialectHandleInsertDialect(MLIR.mlirGetDialectHandle__arith__(), registry);
    MLIR.mlirDialectHandleInsertDialect(MLIR.mlirGetDialectHandle__memref__(), registry);
    MLIR.mlirDialectHandleInsertDialect(MLIR.mlirGetDialectHandle__scf__(), registry);

    if (verbose) {
        std.debug.print("✅ Registered specific dialects\n", .{});
    }

    // Register transform passes we need
    MLIR.mlirRegisterTransformsCanonicalizer();
    MLIR.mlirRegisterAllPasses();

    // Register GPU passes - try to register but don't fail if not available
    // Note: GPU passes might not be available in all MLIR builds
    if (verbose) {
        std.debug.print("✅ Registered canonicalizer pass\n", .{});
        std.debug.print("⚠️  GPU pass registration will be attempted during pipeline parsing\n", .{});
    }

    // Initialize MLIR context with all dialects and passes registered
    const context = MLIR.mlirContextCreateWithRegistry(registry, false);
    defer MLIR.mlirContextDestroy(context);

    // Register LLVM translations for mlirTranslateModuleToLLVMIR
    MLIR.mlirRegisterAllLLVMTranslations(context);

    if (verbose) {
        std.debug.print("✅ Created MLIR context with pre-registered dialects\n", .{});
        std.debug.print("✅ Registered LLVM translations for MLIR→LLVM IR conversion\n", .{});
    }

    if (verbose) {
        std.debug.print("✅ Registered all dialects and passes for parsing\n", .{});
    }

    // Create pass manager for builtin.module operations
    const pass_manager = MLIR.mlirPassManagerCreate(context);
    defer MLIR.mlirPassManagerDestroy(pass_manager);

    // Add canonicalize pass using MLIR C API
    const canonicalize_pass = MLIR.mlirCreateTransformsCanonicalizer();
    MLIR.mlirPassManagerAddOwnedPass(pass_manager, canonicalize_pass);
    const kernel_pass = MLIR.mlirCreateGPUGpuKernelOutlining();
    MLIR.mlirPassManagerAddOwnedPass(pass_manager, kernel_pass);
    const scf_to_cf_pass = MLIR.mlirCreateConversionSCFToControlFlow();
    MLIR.mlirPassManagerAddOwnedPass(pass_manager, scf_to_cf_pass);

    // Get the default operation pass manager and create nested GPU module pass manager
    const default_op_pm = MLIR.mlirPassManagerGetAsOpPassManager(pass_manager);
    const gpu_module_pm = MLIR.mlirOpPassManagerGetNestedUnder(default_op_pm, MLIR.mlirStringRefCreateFromCString("gpu.module"));
    // Use our custom wrapper that supports bare pointer call convention
    const gpu_to_nvvm_pass = MLIR.mlirCreateConversionConvertGpuOpsToNVVMOpsWithBarePtr();
    MLIR.mlirOpPassManagerAddOwnedPass(gpu_module_pm, gpu_to_nvvm_pass);

    // Add NVVM to LLVM conversion pass to the main module pass manager
    const nvvm_to_llvm_pass = MLIR.mlirCreateConversionConvertNVVMToLLVMPass();
    MLIR.mlirPassManagerAddOwnedPass(pass_manager, nvvm_to_llvm_pass);

    // Add finalize MemRef to LLVM conversion pass
    const finalize_memref_pass = MLIR.mlirCreateConversionFinalizeMemRefToLLVMConversionPass();
    MLIR.mlirPassManagerAddOwnedPass(pass_manager, finalize_memref_pass);

    // Add convert Func to LLVM pass with bare pointer call convention
    const func_to_llvm_pass = MLIR.mlirCreateConversionConvertFuncToLLVMPassWithBarePtr();
    MLIR.mlirPassManagerAddOwnedPass(pass_manager, func_to_llvm_pass);

    // Add reconcile unrealized casts pass
    const reconcile_casts_pass = MLIR.mlirCreateConversionReconcileUnrealizedCasts();
    MLIR.mlirPassManagerAddOwnedPass(pass_manager, reconcile_casts_pass);

    if (verbose) {
        std.debug.print("✅ Created pass manager with integrated passes: canonicalize, GPU kernel outlining, SCF to CF, GPU to NVVM (bare ptr), NVVM to LLVM, finalize MemRef to LLVM, convert Func to LLVM (bare ptr), reconcile unrealized casts\n", .{});
    }

    // Parse the actual input MLIR file
    if (verbose) {
        std.debug.print("🔍 About to parse MLIR file...\n", .{});
    }

    const input_str_ref = MLIR.mlirStringRefCreateFromCString(null_terminated_content);

    if (verbose) {
        std.debug.print("🔍 Created string reference {s}, calling mlirModuleCreateParse...\n", .{null_terminated_content});
    }

    const module = MLIR.mlirModuleCreateParse(context, input_str_ref);

    if (MLIR.mlirModuleIsNull(module)) {
        if (verbose) {
            std.debug.print("❌ Failed to parse MLIR - using simple text replacement fallback\n", .{});
        }
        // Fallback: just return input content (no transformation)
        return try allocator.dupe(u8, input_content);
    }
    defer MLIR.mlirModuleDestroy(module);

    if (verbose) {
        std.debug.print("🔍 Parsing completed successfully!\n", .{});
    }

    if (verbose) {
        std.debug.print("✅ Successfully parsed input MLIR file\n", .{});
    }

    // Get module operation for dumping (skip pass for now to avoid crash)
    if (verbose) {
        std.debug.print("🔧 Getting module operation...\n", .{});
    }
    const module_op = MLIR.mlirModuleGetOperation(module);

    if (verbose) {
        std.debug.print("🔧 Skipping pass manager for now to test mlirOperationDump...\n", .{});
    }

    // TODO: Re-enable pass after fixing crash
    _ = MLIR.mlirPassManagerRunOnOp(pass_manager, module_op);

    // Capture stderr output from mlirOperationDump
    if (verbose) {
        std.debug.print("🔍 Capturing MLIR operation dump to file...\n", .{});
    }

    // Create temporary file to capture stderr
    const temp_file_path = "temp_mlir_dump.txt";

    // Save current stderr
    const original_stderr = std.posix.dup(std.posix.STDERR_FILENO) catch |err| {
        std.debug.print("Error duplicating stderr: {}\n", .{err});
        return EmitPtxError.PipelineError;
    };
    defer std.posix.close(original_stderr);

    // Create new file and redirect stderr to it
    const temp_file = std.fs.cwd().createFile(temp_file_path, .{}) catch |err| {
        std.debug.print("Error creating temp file: {}\n", .{err});
        return EmitPtxError.PipelineError;
    };
    defer std.fs.cwd().deleteFile(temp_file_path) catch {};

    const temp_fd = temp_file.handle;

    // Redirect stderr to our temp file
    std.posix.dup2(temp_fd, std.posix.STDERR_FILENO) catch |err| {
        std.debug.print("Error redirecting stderr: {}\n", .{err});
        return EmitPtxError.PipelineError;
    };

    // Now dump the MLIR - this will go to our temp file
    MLIR.mlirOperationDump(module_op);

    // Restore original stderr
    std.posix.dup2(original_stderr, std.posix.STDERR_FILENO) catch |err| {
        std.debug.print("Error restoring stderr: {}\n", .{err});
        return EmitPtxError.PipelineError;
    };

    // Ensure temp file is flushed and closed before reading
    temp_file.close();

    // Read the captured content from temp file
    const captured_mlir = std.fs.cwd().readFileAlloc(allocator, temp_file_path, 1024 * 1024) catch |err| {
        std.debug.print("Error reading captured MLIR: {}\n", .{err});
        return EmitPtxError.PipelineError;
    };

    if (verbose) {
        std.debug.print("✅ Successfully captured MLIR dump\n", .{});
        std.debug.print("   🎯 Used MLIR library for: parse → canonicalize → GPU kernel outlining → SCF to CF → GPU to NVVM (bare ptr) → NVVM to LLVM → finalize MemRef to LLVM → convert Func to LLVM (bare ptr) → reconcile unrealized casts → output\n", .{});
        std.debug.print("   🔍 MLIR content captured from mlirOperationDump successfully!\n", .{});
    }

    return captured_mlir;
}

fn parseArgs(allocator: std.mem.Allocator, output_file_path: *?[]const u8) !Args {
    const args_list = try std.process.argsAlloc(allocator);
    defer std.process.argsFree(allocator, args_list);

    var result = Args{
        .input_file = "",
        .function_name = "",
        .sm_version = 50, // Default SM version
    };

    if (args_list.len < 2) {
        std.debug.print("Error: Missing arguments\n\n", .{});
        printUsage();
        return EmitPtxError.InvalidArguments;
    }

    var i: usize = 1; // Skip program name
    while (i < args_list.len) {
        const arg = args_list[i];

        if (std.mem.eql(u8, arg, "--help") or std.mem.eql(u8, arg, "-h")) {
            result.help = true;
            return result;
        } else if (std.mem.eql(u8, arg, "--verbose") or std.mem.eql(u8, arg, "-v")) {
            result.verbose = true;
        } else if (std.mem.eql(u8, arg, "--sm")) {
            i += 1;
            if (i >= args_list.len) {
                std.debug.print("Error: --sm requires a value\n", .{});
                return EmitPtxError.InvalidArguments;
            }
            result.sm_version = std.fmt.parseInt(u32, args_list[i], 10) catch {
                std.debug.print("Error: Invalid SM version '{s}'\n", .{args_list[i]});
                return EmitPtxError.InvalidArguments;
            };
        } else if (std.mem.eql(u8, arg, "--output") or std.mem.eql(u8, arg, "-o")) {
            i += 1;
            if (i >= args_list.len) {
                std.debug.print("Error: --output requires a value\n", .{});
                return EmitPtxError.InvalidArguments;
            }
            output_file_path.* = try allocator.dupe(u8, args_list[i]);
        } else if (std.mem.startsWith(u8, arg, "--")) {
            std.debug.print("Error: Unknown option '{s}'\n", .{arg});
            return EmitPtxError.InvalidArguments;
        } else {
            // Positional arguments
            if (result.input_file.len == 0) {
                result.input_file = try allocator.dupe(u8, arg);
            } else if (result.function_name.len == 0) {
                result.function_name = try allocator.dupe(u8, arg);
            } else {
                std.debug.print("Error: Too many positional arguments\n", .{});
                return EmitPtxError.InvalidArguments;
            }
        }
        i += 1;
    }

    if (result.input_file.len == 0) {
        std.debug.print("Error: Missing input file\n", .{});
        return EmitPtxError.InvalidArguments;
    }

    if (result.function_name.len == 0) {
        std.debug.print("Error: Missing function name\n", .{});
        return EmitPtxError.InvalidArguments;
    }

    return result;
}

fn findFunction(ast: parser.ASTNode, target_name: []const u8) !@TypeOf(@as(parser.ASTNode, undefined).function_declaration) {
    switch (ast) {
        .program => |program| {
            for (program.statements) |stmt| {
                switch (stmt) {
                    .function_declaration => |func| {
                        if (std.mem.eql(u8, func.name, target_name)) {
                            return func;
                        }
                    },
                    else => {},
                }
            }
        },
        else => {},
    }

    std.debug.print("Error: Function '{s}' not found\n", .{target_name});
    return EmitPtxError.FunctionNotFound;
}

fn printUsage() void {
    std.debug.print("Usage: emit_ptx [OPTIONS] <input_file> <function_name>\n\n", .{});
    std.debug.print("Generate PTX code for a specific GPU function from a toy language file\n\n", .{});
    std.debug.print("Arguments:\n", .{});
    std.debug.print("  <input_file>     Path to the toy language source file\n", .{});
    std.debug.print("  <function_name>  Name of the function to generate PTX for\n\n", .{});
    std.debug.print("Options:\n", .{});
    std.debug.print("  --sm <version>   Target SM architecture (default: 50)\n", .{});
    std.debug.print("                   Examples: 50, 60, 70, 75, 80, 86, 89\n", .{});
    std.debug.print("  -v, --verbose    Enable verbose output\n", .{});
    std.debug.print("  -h, --help       Show this help message\n", .{});
    std.debug.print("  -o, --output <file>  Write output to <file> instead of stdout\n", .{});
    std.debug.print("\nExamples:\n", .{});
    std.debug.print("  emit_ptx kernel.toy gpu_vector_add\n", .{});
    std.debug.print("  emit_ptx --sm 89 --verbose kernel.toy gpu_matrix_mul\n", .{});
    std.debug.print("  emit_ptx -o output.ptx kernel.toy gpu_vector_add\n", .{});
    std.debug.print("  emit_ptx --output kernel.ptx --sm 80 --verbose kernel.toy gpu_add\n", .{});
    std.debug.print("\nNote: Currently falls back to static GPU MLIR from simple_vector_add_gpu.mlir\n", .{});
}

test "emit_ptx basic functionality" {
    // Basic test to ensure the tool compiles

    // Test argument parsing with help flag
    const args = Args{
        .input_file = "test.toy",
        .function_name = "gpu_test",
        .help = true,
    };

    try std.testing.expect(args.help == true);
    try std.testing.expect(std.mem.eql(u8, args.input_file, "test.toy"));
    try std.testing.expect(std.mem.eql(u8, args.function_name, "gpu_test"));
}
