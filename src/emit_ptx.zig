const std = @import("std");
const lexer = @import("lexer.zig");
const parser = @import("parser.zig");

const EmitPtxError = error{
    FileNotFound,
    ParseError,
    FunctionNotFound,
    PipelineError,
    InvalidArguments,
    OutputFileError,
} || std.mem.Allocator.Error;

const Args = struct {
    input_file: []const u8,
    function_name: []const u8,
    sm_version: u32 = 50, // Default to SM 5.0
    verbose: bool = false,
    help: bool = false,
};

pub fn main() !void {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    // Parse command line arguments
    var output_file_path: ?[]const u8 = null;
    const args = try parseArgs(allocator, &output_file_path);
    defer if (output_file_path) |path| allocator.free(path);

    if (args.help) {
        printUsage();
        return;
    }

    // Clean up duplicated strings at the end
    defer allocator.free(args.input_file);
    defer allocator.free(args.function_name);
    
    if (args.verbose) {
        std.debug.print("Emit PTX Tool (using MLIR pipeline)\n", .{});
        std.debug.print("Input file: {s}\n", .{args.input_file});
        std.debug.print("Function: {s}\n", .{args.function_name});
        std.debug.print("SM version: {d}\n", .{args.sm_version});
        if (output_file_path) |output_file| {
            std.debug.print("Output file: {s}\n", .{output_file});
        } else {
            std.debug.print("Output: stdout\n", .{});
        }
    }

    // Try to generate PTX using our pipeline
    const ptx_code = generatePTXFromPipeline(allocator, args) catch |err| {
        if (args.verbose) {
            std.debug.print("Pipeline failed: {}, falling back to static GPU MLIR\n", .{err});
        }
        const fallback_ptx = try generatePTXFromStatic(allocator, args);
        defer allocator.free(fallback_ptx);
        
        // Generate PTX header
        const header = try std.fmt.allocPrint(allocator,
            "// Generated PTX for function: {s}\n// Target: SM {d}\n// Generated by DCC emit_ptx tool using MLIR pipeline (fallback)\n\n",
            .{ args.function_name, args.sm_version }
        );
        defer allocator.free(header);
        
        // Output the PTX code
        try outputPTX(allocator, output_file_path, header, fallback_ptx, args.verbose);
        return;
    };
    defer allocator.free(ptx_code);

    // Generate PTX header
    const header = try std.fmt.allocPrint(allocator,
        "// Generated PTX for function: {s}\n// Target: SM {d}\n// Generated by DCC emit_ptx tool using MLIR pipeline\n\n",
        .{ args.function_name, args.sm_version }
    );
    defer allocator.free(header);

    // Output the PTX code
    try outputPTX(allocator, output_file_path, header, ptx_code, args.verbose);
}

fn outputPTX(allocator: std.mem.Allocator, output_file: ?[]const u8, header: []const u8, ptx_code: []const u8, verbose: bool) !void {
    const full_content = try std.fmt.allocPrint(allocator, "{s}{s}", .{ header, ptx_code });
    defer allocator.free(full_content);

    if (output_file) |file_path| {
        // Write to file
        std.fs.cwd().writeFile(.{ .sub_path = file_path, .data = full_content }) catch |err| {
            std.debug.print("Error writing to output file '{s}': {}\n", .{ file_path, err });
            return EmitPtxError.OutputFileError;
        };
        if (verbose) {
            std.debug.print("✅ PTX written to: {s}\n", .{file_path});
        }
    } else {
        // Write to stdout
        std.debug.print("{s}", .{full_content});
    }
}

fn generatePTXFromPipeline(allocator: std.mem.Allocator, args: Args) ![]const u8 {
    if (args.verbose) {
        std.debug.print("Attempting to parse input file and generate custom GPU MLIR...\n", .{});
    }

    // Read and parse input file
    const source_code = std.fs.cwd().readFileAlloc(allocator, args.input_file, 1024 * 1024) catch |err| {
        switch (err) {
            error.FileNotFound => {
                std.debug.print("Error: File '{s}' not found\n", .{args.input_file});
                return EmitPtxError.FileNotFound;
            },
            else => return err,
        }
    };
    defer allocator.free(source_code);

    // Tokenize
    var lex = lexer.Lexer.init(allocator, source_code);
    const tokens = try lex.tokenize();
    defer allocator.free(tokens);

    // Parse
    var parse = parser.Parser.init(allocator, tokens, source_code);
    const ast = parse.parse() catch |err| {
        std.debug.print("Parse error: {}\n", .{err});
        return EmitPtxError.ParseError;
    };
    defer parser.freeAST(allocator, ast);

    // Find the specified function
    const func_decl = try findFunction(ast, args.function_name);

    if (args.verbose) {
        std.debug.print("Found function: {s}\n", .{func_decl.name});
    }

    // For now, if we reach here, we'll fall back to static
    // TODO: Implement dynamic GPU MLIR generation from AST
    return EmitPtxError.PipelineError;
}

fn generatePTXFromStatic(allocator: std.mem.Allocator, args: Args) ![]const u8 {
    if (args.verbose) {
        std.debug.print("Using static GPU MLIR from simple_vector_add_gpu.mlir\n", .{});
    }

    // Step 1: Canonicalize
    if (args.verbose) std.debug.print("📝 Canonicalizing...\n", .{});
    try runCommand(allocator, &[_][]const u8{ "mlir-opt", "simple_vector_add_gpu.mlir", "--canonicalize", "-o", "step1_canon.mlir" });

    // Step 2: GPU kernel outlining
    if (args.verbose) std.debug.print("🎯 GPU kernel outlining...\n", .{});
    try runCommand(allocator, &[_][]const u8{ "mlir-opt", "step1_canon.mlir", "--gpu-kernel-outlining", "-o", "step2_outlined.mlir" });

    // Step 3: Convert SCF to CF
    if (args.verbose) std.debug.print("🔀 Converting SCF to CF...\n", .{});
    try runCommand(allocator, &[_][]const u8{ "mlir-opt", "step2_outlined.mlir", "--convert-scf-to-cf", "-o", "step3_cf.mlir" });

    // Step 4: Convert GPU to NVVM with bare pointer optimization
    if (args.verbose) std.debug.print("🎮 Converting GPU to NVVM (with bare pointers)...\n", .{});
    try runCommand(allocator, &[_][]const u8{ "mlir-opt", "step3_cf.mlir", "--convert-gpu-to-nvvm=use-bare-ptr-memref-call-conv", "-o", "step4_nvvm.mlir" });

    // Step 5: Complete the conversion and clean up unrealized casts
    if (args.verbose) std.debug.print("🧹 Finalizing conversion to LLVM (with bare pointers)...\n", .{});
    try runCommand(allocator, &[_][]const u8{ 
        "mlir-opt", "step4_nvvm.mlir", 
        "--convert-nvvm-to-llvm",
        "--finalize-memref-to-llvm", 
        "--convert-func-to-llvm=use-bare-ptr-memref-call-conv", 
        "--reconcile-unrealized-casts", 
        "-o", "step5_final.mlir" 
    });

    // Step 6: Extract kernel function for standalone compilation
    if (args.verbose) std.debug.print("🔧 Extracting kernel function...\n", .{});
    try extractKernelFunction(allocator, "step5_final.mlir", "standalone_kernel.mlir");

    // Step 7: Fix NVVM operations for mlir-translate
    if (args.verbose) std.debug.print("🔧 Fixing NVVM operations...\n", .{});
    try fixNVVMOperations(allocator, "standalone_kernel.mlir", "standalone_fixed.mlir");
    
    // Step 8: Translate MLIR to LLVM IR  
    if (args.verbose) std.debug.print("🔄 Translating MLIR to LLVM IR...\n", .{});
    try runCommand(allocator, &[_][]const u8{ "mlir-translate", "standalone_fixed.mlir", "--mlir-to-llvmir", "-o", "kernel.ll" });

    // Step 9: Compile LLVM IR to PTX using llc
    if (args.verbose) std.debug.print("🎯 Compiling LLVM IR to PTX...\n", .{});
    const mcpu_arg = try std.fmt.allocPrint(allocator, "-mcpu=sm_{d}", .{args.sm_version});
    defer allocator.free(mcpu_arg);
    
    try runCommand(allocator, &[_][]const u8{ 
        "llc", "kernel.ll", 
        "-march=nvptx64", 
        mcpu_arg,
        "-o", "kernel.ptx" 
    });

    // Step 10: Read the generated PTX
    if (args.verbose) std.debug.print("📖 Reading generated PTX...\n", .{});
    const ptx_content = std.fs.cwd().readFileAlloc(allocator, "kernel.ptx", 1024 * 1024) catch |err| {
        std.debug.print("Error reading generated PTX: {}\n", .{err});
        return EmitPtxError.PipelineError;
    };

    // Clean up temporary files
    if (!args.verbose) {
        std.fs.cwd().deleteFile("standalone_fixed.mlir") catch {};
        std.fs.cwd().deleteFile("kernel.ll") catch {};
        std.fs.cwd().deleteFile("kernel.ptx") catch {};
    }

    if (args.verbose) std.debug.print("✅ PTX generation complete\n", .{});
    return ptx_content;
}

fn runCommand(allocator: std.mem.Allocator, argv: []const []const u8) !void {
    const result = std.process.Child.run(.{
        .allocator = allocator,
        .argv = argv,
    }) catch |err| {
        std.debug.print("Error running command: {}\n", .{err});
        return EmitPtxError.PipelineError;
    };
    defer allocator.free(result.stdout);
    defer allocator.free(result.stderr);

    if (result.term != .Exited or result.term.Exited != 0) {
        std.debug.print("Command failed: {s}\n", .{argv[0]});
        if (result.stderr.len > 0) {
            std.debug.print("Error: {s}\n", .{result.stderr});
        }
        return EmitPtxError.PipelineError;
    }
}

fn extractKernelFunction(allocator: std.mem.Allocator, input_file: []const u8, output_file: []const u8) !void {
    // Read the input file
    const content = std.fs.cwd().readFileAlloc(allocator, input_file, 1024 * 1024) catch |err| {
        std.debug.print("Error reading file: {}\n", .{err});
        return EmitPtxError.PipelineError;
    };
    defer allocator.free(content);

    // Find the kernel function and extract it
    const kernel_start = std.mem.indexOf(u8, content, "llvm.func @gpu_add_kernel") orelse {
        std.debug.print("Error: Could not find kernel function\n", .{});
        return EmitPtxError.PipelineError;
    };
    
    const kernel_end = std.mem.indexOf(u8, content[kernel_start..], "  }") orelse {
        std.debug.print("Error: Could not find end of kernel function\n", .{});
        return EmitPtxError.PipelineError;
    };
    
    // Extract the kernel function
    const kernel_function = content[kernel_start..kernel_start + kernel_end + 3];
    
    // Remove the gpu.kernel attribute as it's not needed in standalone module
    const kernel_with_attrs = std.mem.replacementSize(u8, kernel_function, "gpu.kernel, ", "");
    const cleaned_kernel = try allocator.alloc(u8, kernel_with_attrs);
    defer allocator.free(cleaned_kernel);
    _ = std.mem.replace(u8, kernel_function, "gpu.kernel, ", "", cleaned_kernel);
    
    // Also remove if it's at the end
    const kernel_with_attrs2 = std.mem.replacementSize(u8, cleaned_kernel, ", gpu.kernel", "");
    const cleaned_kernel2 = try allocator.alloc(u8, kernel_with_attrs2);
    defer allocator.free(cleaned_kernel2);
    _ = std.mem.replace(u8, cleaned_kernel, ", gpu.kernel", "", cleaned_kernel2);
    
    // Final cleanup - use the final cleaned version
    const final_kernel = try allocator.dupe(u8, cleaned_kernel2);
    
    // Create a standalone module with the kernel function
    const standalone_module = try std.fmt.allocPrint(allocator,
        \\module attributes {{nvvm.target = "cuda"}} {{
        \\  {s}
        \\  llvm.func @llvm.nvvm.read.ptx.sreg.tid.x() -> i32 attributes {{passthrough = ["nounwind", "readnone"]}}
        \\  llvm.func @llvm.nvvm.read.ptx.sreg.ntid.x() -> i32 attributes {{passthrough = ["nounwind", "readnone"]}}
        \\  llvm.func @llvm.nvvm.read.ptx.sreg.ctaid.x() -> i32 attributes {{passthrough = ["nounwind", "readnone"]}}
        \\}}
        \\
    , .{final_kernel});
    defer allocator.free(final_kernel);
    defer allocator.free(standalone_module);
    
    // Write the extracted kernel to output file
    std.fs.cwd().writeFile(.{ .sub_path = output_file, .data = standalone_module }) catch |err| {
        std.debug.print("Error writing file: {}\n", .{err});
        return EmitPtxError.PipelineError;
    };
}

fn fixNVVMOperations(allocator: std.mem.Allocator, input_file: []const u8, output_file: []const u8) !void {
    // Read the input MLIR file
    const input_content = std.fs.cwd().readFileAlloc(allocator, input_file, 1024 * 1024) catch |err| {
        std.debug.print("Error reading input MLIR file: {}\n", .{err});
        return EmitPtxError.PipelineError;
    };
    defer allocator.free(input_content);
    
    // Replace NVVM operations with LLVM intrinsic calls
    const content1 = std.mem.replaceOwned(u8, allocator, input_content, "nvvm.read.ptx.sreg.ctaid.x : i32", "llvm.call @llvm.nvvm.read.ptx.sreg.ctaid.x() : () -> i32") catch |err| {
        std.debug.print("Error replacing NVVM operations: {}\n", .{err});
        return EmitPtxError.PipelineError;
    };
    defer allocator.free(content1);
    
    const content2 = std.mem.replaceOwned(u8, allocator, content1, "nvvm.read.ptx.sreg.ntid.x : i32", "llvm.call @llvm.nvvm.read.ptx.sreg.ntid.x() : () -> i32") catch |err| {
        std.debug.print("Error replacing NVVM operations: {}\n", .{err});
        return EmitPtxError.PipelineError;
    };
    defer allocator.free(content2);
    
    const content3 = std.mem.replaceOwned(u8, allocator, content2, "nvvm.read.ptx.sreg.tid.x : i32", "llvm.call @llvm.nvvm.read.ptx.sreg.tid.x() : () -> i32") catch |err| {
        std.debug.print("Error replacing NVVM operations: {}\n", .{err});
        return EmitPtxError.PipelineError;
    };
    defer allocator.free(content3);
    
    // Remove invalid function attributes and nvvm.target
    const content4 = std.mem.replaceOwned(u8, allocator, content3, "attributes {passthrough = [\"nounwind\", \"readnone\"]}", "") catch |err| {
        std.debug.print("Error removing invalid attributes: {}\n", .{err});
        return EmitPtxError.PipelineError;
    };
    defer allocator.free(content4);
    
    const clean_content = std.mem.replaceOwned(u8, allocator, content4, "module attributes {nvvm.target = \"cuda\"} {", "module {") catch |err| {
        std.debug.print("Error removing nvvm.target: {}\n", .{err});
        return EmitPtxError.PipelineError;
    };
    defer allocator.free(clean_content);
    
    // Write the fixed MLIR file
    var file = std.fs.cwd().createFile(output_file, .{}) catch |err| {
        std.debug.print("Error creating output MLIR file: {}\n", .{err});
        return EmitPtxError.PipelineError;
    };
    defer file.close();
    
    try file.writeAll(clean_content);
}

fn parseArgs(allocator: std.mem.Allocator, output_file_path: *?[]const u8) !Args {
    const args_list = try std.process.argsAlloc(allocator);
    defer std.process.argsFree(allocator, args_list);

    var result = Args{
        .input_file = "",
        .function_name = "",
        .sm_version = 50, // Default SM version
    };

    if (args_list.len < 2) {
        std.debug.print("Error: Missing arguments\n\n", .{});
        printUsage();
        return EmitPtxError.InvalidArguments;
    }

    var i: usize = 1; // Skip program name
    while (i < args_list.len) {
        const arg = args_list[i];

        if (std.mem.eql(u8, arg, "--help") or std.mem.eql(u8, arg, "-h")) {
            result.help = true;
            return result;
        } else if (std.mem.eql(u8, arg, "--verbose") or std.mem.eql(u8, arg, "-v")) {
            result.verbose = true;
        } else if (std.mem.eql(u8, arg, "--sm")) {
            i += 1;
            if (i >= args_list.len) {
                std.debug.print("Error: --sm requires a value\n", .{});
                return EmitPtxError.InvalidArguments;
            }
            result.sm_version = std.fmt.parseInt(u32, args_list[i], 10) catch {
                std.debug.print("Error: Invalid SM version '{s}'\n", .{args_list[i]});
                return EmitPtxError.InvalidArguments;
            };
        } else if (std.mem.eql(u8, arg, "--output") or std.mem.eql(u8, arg, "-o")) {
            i += 1;
            if (i >= args_list.len) {
                std.debug.print("Error: --output requires a value\n", .{});
                return EmitPtxError.InvalidArguments;
            }
            output_file_path.* = try allocator.dupe(u8, args_list[i]);
        } else if (std.mem.startsWith(u8, arg, "--")) {
            std.debug.print("Error: Unknown option '{s}'\n", .{arg});
            return EmitPtxError.InvalidArguments;
        } else {
            // Positional arguments
            if (result.input_file.len == 0) {
                result.input_file = try allocator.dupe(u8, arg);
            } else if (result.function_name.len == 0) {
                result.function_name = try allocator.dupe(u8, arg);
            } else {
                std.debug.print("Error: Too many positional arguments\n", .{});
                return EmitPtxError.InvalidArguments;
            }
        }
        i += 1;
    }

    if (result.input_file.len == 0) {
        std.debug.print("Error: Missing input file\n", .{});
        return EmitPtxError.InvalidArguments;
    }

    if (result.function_name.len == 0) {
        std.debug.print("Error: Missing function name\n", .{});
        return EmitPtxError.InvalidArguments;
    }

    return result;
}

fn findFunction(ast: parser.ASTNode, target_name: []const u8) !@TypeOf(@as(parser.ASTNode, undefined).function_declaration) {
    switch (ast) {
        .program => |program| {
            for (program.statements) |stmt| {
                switch (stmt) {
                    .function_declaration => |func| {
                        if (std.mem.eql(u8, func.name, target_name)) {
                            return func;
                        }
                    },
                    else => {},
                }
            }
        },
        else => {},
    }
    
    std.debug.print("Error: Function '{s}' not found\n", .{target_name});
    return EmitPtxError.FunctionNotFound;
}

fn printUsage() void {
    std.debug.print("Usage: emit_ptx [OPTIONS] <input_file> <function_name>\n\n", .{});
    std.debug.print("Generate PTX code for a specific GPU function from a toy language file\n\n", .{});
    std.debug.print("Arguments:\n", .{});
    std.debug.print("  <input_file>     Path to the toy language source file\n", .{});
    std.debug.print("  <function_name>  Name of the function to generate PTX for\n\n", .{});
    std.debug.print("Options:\n", .{});
    std.debug.print("  --sm <version>   Target SM architecture (default: 50)\n", .{});
    std.debug.print("                   Examples: 50, 60, 70, 75, 80, 86, 89\n", .{});
    std.debug.print("  -v, --verbose    Enable verbose output\n", .{});
    std.debug.print("  -h, --help       Show this help message\n", .{});
    std.debug.print("  -o, --output <file>  Write output to <file> instead of stdout\n", .{});
    std.debug.print("\nExamples:\n", .{});
    std.debug.print("  emit_ptx kernel.toy gpu_vector_add\n", .{});
    std.debug.print("  emit_ptx --sm 89 --verbose kernel.toy gpu_matrix_mul\n", .{});
    std.debug.print("  emit_ptx -o output.ptx kernel.toy gpu_vector_add\n", .{});
    std.debug.print("  emit_ptx --output kernel.ptx --sm 80 --verbose kernel.toy gpu_add\n", .{});
    std.debug.print("\nNote: Currently falls back to static GPU MLIR from simple_vector_add_gpu.mlir\n", .{});
}

test "emit_ptx basic functionality" {
    // Basic test to ensure the tool compiles
    
    // Test argument parsing with help flag
    const args = Args{
        .input_file = "test.toy",
        .function_name = "gpu_test",
        .help = true,
    };
    
    try std.testing.expect(args.help == true);
    try std.testing.expect(std.mem.eql(u8, args.input_file, "test.toy"));
    try std.testing.expect(std.mem.eql(u8, args.function_name, "gpu_test"));
} 